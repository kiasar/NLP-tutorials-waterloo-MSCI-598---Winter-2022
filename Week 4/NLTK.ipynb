{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK"
      ],
      "metadata": {
        "id": "EZAYWOXNEn8J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4dOJmwRuxd1C"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import nltk.tokenize as tok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chMw3Dv_xtXT",
        "outputId": "3a7b1ef8-4d8a-47a6-9b5d-2bcb3cdfab10"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Let me tell you a joke. What do you call a fake noodle!? An impasta!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLYFsxSQxh6C",
        "outputId": "47754d22-304a-46b4-a691-7aac714ac0f3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Let', 'me', 'tell', 'you', 'a', 'joke', '.', 'What', 'do', 'you', 'call', 'a', 'fake', 'noodle', '!', '?', 'An', 'impasta', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can tokenize by word\n",
        "print(tok.word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_S-8qCoxq29",
        "outputId": "ff55d8e8-5a53-4b36-d45d-366373252f24"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Let', 'me', 'tell', 'you', 'a', 'joke', '.', 'What', 'do', 'you', 'call', 'a', 'fake', 'noodle', '!', '?', 'An', 'impasta', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can tokenize by sentences\n",
        "print(tok.sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzJvGrWRyZ3V",
        "outputId": "705a492b-db47-4e09-b458-127f10d7a44a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Let me tell you a joke.', 'What do you call a fake noodle!?', 'An impasta!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlj67qUrygCW",
        "outputId": "829dc088-068e-43d1-c795-e508a3ba8e6c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can tag your tokens like this. VB= verb, NN = noun etc.\n",
        "\n",
        "tokens = tok.word_tokenize(text)\n",
        "nltk.pos_tag(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpa3cbM6zHTs",
        "outputId": "bbfa45b4-af54-4a66-9a4a-e37ea0056943"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Let', 'VB'),\n",
              " ('me', 'PRP'),\n",
              " ('tell', 'VB'),\n",
              " ('you', 'PRP'),\n",
              " ('a', 'DT'),\n",
              " ('joke', 'NN'),\n",
              " ('.', '.'),\n",
              " ('What', 'WP'),\n",
              " ('do', 'VBP'),\n",
              " ('you', 'PRP'),\n",
              " ('call', 'VB'),\n",
              " ('a', 'DT'),\n",
              " ('fake', 'JJ'),\n",
              " ('noodle', 'NN'),\n",
              " ('!', '.'),\n",
              " ('?', '.'),\n",
              " ('An', 'DT'),\n",
              " ('impasta', 'NN'),\n",
              " ('!', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can do Stemming like this\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "e_words= [\"studies\", \"studying\", \"tell\", \"told\", \"car\", \"cars\", \"see\", \"seeing\"]\n",
        "ps =PorterStemmer()\n",
        "for w in e_words:\n",
        "    rootWord=ps.stem(w)\n",
        "    print(rootWord)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtvdV_KcY1kQ",
        "outputId": "ca485912-b2a0-412f-8968-5b9ebff9ca0c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "studi\n",
            "studi\n",
            "tell\n",
            "told\n",
            "car\n",
            "car\n",
            "see\n",
            "see\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BPcuHPaZ9fv",
        "outputId": "9e66efcf-9f54-4f2b-bae5-19c8f5f5333f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can do Stemming like this\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "e_words= [\"studies\", \"studying\", \"tell\", \"told\", \"car\", \"cars\", \"see\", \"seeing\"]\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "for w in e_words:\n",
        "    rootWord = wordnet_lemmatizer.lemmatize(w)\n",
        "    print(rootWord)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-4uuV0KY8_W",
        "outputId": "1a406ff5-25bb-422a-c091-63fba50bf549"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "study\n",
            "studying\n",
            "tell\n",
            "told\n",
            "car\n",
            "car\n",
            "see\n",
            "seeing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# there is another python package \"SpaCy\" you can look for tokenization."
      ],
      "metadata": {
        "id": "D1OPWidvZugH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}